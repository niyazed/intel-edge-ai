{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Non-Frozen Models to the Model Optimizer\n",
    "\n",
    "### NOTE:  `<INSTALL_DIR>  =  /opt/intel/openvino/deployment_tools/model_optimizer`\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "> This is pretty long! I would suggest considering setting a path environment variable for the Model Optimizer if you are working locally on a Linux-based machine. You could do something like this:\n",
    "- export MOD_OPT=/opt/intel/openvino/deployment_tools/model_optimizer`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "There are three ways to store non-frozen TensorFlow models and load them to the Model Optimizer:\n",
    "\n",
    "- Checkpoint:\n",
    "\n",
    "In this case, a model consists of two files:\n",
    "\n",
    "inference_graph.pb or inference_graph.pbtxt\n",
    "checkpoint_file.ckpt\n",
    "If you do not have an inference graph file, refer to Freezing Custom Models in Python.\n",
    "\n",
    "To convert such TensorFlow model:\n",
    "\n",
    "Go to the `<INSTALL_DIR>/deployment_tools/model_optimizer` directory\n",
    "Run the `mo_tf.py` script with the path to the checkpoint file to convert a model:\n",
    "If input model is in .pb format:\n",
    "`python3 mo_tf.py --input_model <INFERENCE_GRAPH>.pb --input_checkpoint <INPUT_CHECKPOINT>`\n",
    "If input model is in .pbtxt format:\n",
    "`python3 mo_tf.py --input_model <INFERENCE_GRAPH>.pbtxt --input_checkpoint <INPUT_CHECKPOINT> --input_model_is_text`\n",
    "\n",
    "- MetaGraph:\n",
    "\n",
    "In this case, a model consists of three or four files stored in the same directory:\n",
    "```\n",
    "model_name.meta\n",
    "model_name.index\n",
    "model_name.data-00000-of-00001 (digit part may vary)\n",
    "checkpoint (optional)\n",
    "```\n",
    "To convert such TensorFlow model:\n",
    "\n",
    "Go to the <INSTALL_DIR>/deployment_tools/model_optimizer directory\n",
    "Run the `mo_tf.py` script with a path to the MetaGraph .meta file to convert a model:\n",
    "`python3 mo_tf.py --input_meta_graph <INPUT_META_GRAPH>.meta`\n",
    "\n",
    "- SavedModel:\n",
    "\n",
    "In this case, a model consists of a special directory with a .pb file and several subfolders: variables, assets, and assets.extra. For more information about the SavedModel directory, refer to the README file in the TensorFlow repository.\n",
    "\n",
    "To convert such TensorFlow model:\n",
    "\n",
    "Go to the `<INSTALL_DIR>/deployment_tools/model_optimizer` directory\n",
    "Run the `mo_tf.py` script with a path to the SavedModel directory to convert a model:\n",
    "`python3 mo_tf.py --saved_model_dir <SAVED_MODEL_DIRECTORY>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# How to Convert a Model\n",
    "\n",
    "Link - https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_Object_Detection_API_Models.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download the model using \n",
    "```sh\n",
    "> wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz\n",
    "``` \n",
    "\n",
    "\n",
    "- Extract\n",
    "```sh\n",
    "> tar -xvf ssd_mobilenet_v2_coco_2018_03_29.tar.gz\n",
    "```\n",
    "\n",
    "\n",
    "- Run this CLI command\n",
    "```sh\n",
    "> python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --input_model=ssd_mobilenet_v2_coco_2018_03_29/frozen_inference_graph.pb --tensorflow_use_custom_operations_config /opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json --tensorflow_object_detection_api_pipeline_config ssd_mobilenet_v2_coco_2018_03_29/pipeline.config --reverse_input_channels\n",
    "```\n",
    "        ....\n",
    "        ....\n",
    "        \n",
    "        [ SUCCESS ] Generated IR model.\n",
    "        [ SUCCESS ] XML file: /home/workspace/./frozen_inference_graph.xml\n",
    "        [ SUCCESS ] BIN file: /home/workspace/./frozen_inference_graph.bin\n",
    "        [ SUCCESS ] Total execution time: 75.28 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
